{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn import preprocessing, decomposition, metrics, impute\n",
    "import torch, torchvision\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from umap import UMAP\n",
    "import seaborn as sns\n",
    "import colorcet as cc\n",
    "import random\n",
    "\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_random_seed(21)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def train_cls(exp_folder, n_labels, train_X, train_Y, test_X, test_Y, mode, model_save_path):\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(672 if mode == \"cp\" else 2048, 256),\n",
    "        nn.BatchNorm1d(256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(256, n_labels),\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters())\n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(train_X).float(), torch.from_numpy(train_Y).long()\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "    test_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(test_X).float(), torch.from_numpy(test_Y).long()\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "    best_accuracy = 0\n",
    "    accuracies = []\n",
    "    for i in range(500):\n",
    "        model.train()\n",
    "        running_total_loss = 0\n",
    "        for _, sample in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            op = model(sample[0])\n",
    "            loss = criterion(op, sample[1])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_total_loss += loss.item()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            model.eval()\n",
    "            accuracy = 0\n",
    "            for _, sample in enumerate(test_loader):\n",
    "                op = model(sample[0])\n",
    "                loss = criterion(op, sample[1])\n",
    "                accuracy += (torch.argmax(op, dim=1) == sample[1]).sum().item()\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                torch.save(model.state_dict(), os.path.join(exp_folder, model_save_path))\n",
    "            print(\n",
    "                \"Epoch: {}, Loss: {}, Accuracy: {}\".format(\n",
    "                    i,\n",
    "                    running_total_loss / len(test_loader.dataset),\n",
    "                    accuracy / len(test_loader.dataset),\n",
    "                )\n",
    "            )\n",
    "            accuracies.append(accuracy / len(test_loader.dataset))\n",
    "\n",
    "    print(\"Best Accuracy:\", np.max(accuracies))\n",
    "    model.load_state_dict(torch.load(os.path.join(exp_folder, model_save_path)))\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        outputs = []\n",
    "        targets = []\n",
    "        for _, sample in enumerate(test_loader):\n",
    "            op = model(sample[0])\n",
    "            outputs.append(torch.argmax(op, dim=1).cpu().numpy())\n",
    "            targets.append(sample[1].cpu().numpy())\n",
    "    return outputs, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/proj/haste_berzelius/exps/specs_non_grit_based/bf_exps_1_split1', '/proj/haste_berzelius/exps/specs_non_grit_based/bf_exps_1_split2', '/proj/haste_berzelius/exps/specs_non_grit_based/bf_exps_1_split3', '/proj/haste_berzelius/exps/specs_non_grit_based/bf_exps_1_split4', '/proj/haste_berzelius/exps/specs_non_grit_based/bf_exps_1_split5']\n"
     ]
    }
   ],
   "source": [
    "bf_folders = sorted(glob.glob(\"/proj/haste_berzelius/exps/specs_non_grit_based/*bf*\"))[:5]\n",
    "exp_cat = \"bf_10cls_basic_aug_dmsonorm_750e_sgd_seed21\"\n",
    "print(bf_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/proj/haste_berzelius/exps/specs_non_grit_based/bf_exps_1_split1/bf_10cls_basic_aug_dmsonorm_750e_sgd_seed21/ResNet_resnet50 True\n",
      "Epoch: 0, Loss: 0.02550233963677447, Accuracy: 0.5115907274180655\n",
      "Epoch: 50, Loss: 0.0018486085591270483, Accuracy: 0.7434052757793765\n",
      "Epoch: 100, Loss: 0.001087305678738107, Accuracy: 0.7490007993605116\n",
      "Epoch: 150, Loss: 0.0011968900033419462, Accuracy: 0.7585931254996003\n",
      "Epoch: 200, Loss: 0.0006696376741099225, Accuracy: 0.759392486011191\n",
      "Epoch: 250, Loss: 0.0008281666337312173, Accuracy: 0.7689848121502798\n",
      "Epoch: 300, Loss: 0.0009238933377581344, Accuracy: 0.7689848121502798\n",
      "Epoch: 350, Loss: 0.0007687285706269846, Accuracy: 0.7657873701039168\n",
      "Epoch: 400, Loss: 0.000500211365622201, Accuracy: 0.7633892885691447\n",
      "Epoch: 450, Loss: 0.00029570337993165046, Accuracy: 0.7705835331734612\n",
      "Best Accuracy: 0.7705835331734612\n",
      "/proj/haste_berzelius/exps/specs_non_grit_based/bf_exps_1_split2/bf_10cls_basic_aug_dmsonorm_750e_sgd_seed21/ResNet_resnet50 True\n",
      "Epoch: 0, Loss: 0.02368191970843021, Accuracy: 0.5490654205607477\n",
      "Epoch: 50, Loss: 0.0014655587214082943, Accuracy: 0.7990654205607477\n",
      "Epoch: 100, Loss: 0.0007406858846289903, Accuracy: 0.8193146417445483\n",
      "Epoch: 150, Loss: 0.0005243746302673749, Accuracy: 0.807632398753894\n",
      "Epoch: 200, Loss: 0.0005205060243664594, Accuracy: 0.8130841121495327\n",
      "Epoch: 250, Loss: 0.0004231423325256395, Accuracy: 0.8146417445482866\n",
      "Epoch: 300, Loss: 0.000412245479055515, Accuracy: 0.8286604361370716\n",
      "Epoch: 350, Loss: 0.0003349287214187029, Accuracy: 0.8161993769470405\n",
      "Epoch: 400, Loss: 0.00020017548293060975, Accuracy: 0.8193146417445483\n",
      "Epoch: 450, Loss: 0.0001754659653069724, Accuracy: 0.8426791277258567\n",
      "Best Accuracy: 0.8426791277258567\n",
      "/proj/haste_berzelius/exps/specs_non_grit_based/bf_exps_1_split3/bf_10cls_basic_aug_dmsonorm_750e_sgd_seed21/ResNet_resnet50 True\n",
      "Epoch: 0, Loss: 0.024405104665172962, Accuracy: 0.5083399523431295\n",
      "Epoch: 50, Loss: 0.0027041634495907877, Accuracy: 0.7426528991262907\n",
      "Epoch: 100, Loss: 0.0026307920433397422, Accuracy: 0.7505957108816521\n",
      "Epoch: 150, Loss: 0.0012200302559056862, Accuracy: 0.7593328038125496\n",
      "Epoch: 200, Loss: 0.0012194137573478903, Accuracy: 0.7529785544082606\n",
      "Epoch: 250, Loss: 0.0009646595080662001, Accuracy: 0.7760127084988085\n",
      "Epoch: 300, Loss: 0.0010765019458044702, Accuracy: 0.7553613979348689\n",
      "Epoch: 350, Loss: 0.00044450870729136597, Accuracy: 0.7823669579030977\n",
      "Epoch: 400, Loss: 0.001502444856667727, Accuracy: 0.767275615567911\n",
      "Epoch: 450, Loss: 0.0006702476276515305, Accuracy: 0.7776012708498808\n",
      "Best Accuracy: 0.7823669579030977\n",
      "/proj/haste_berzelius/exps/specs_non_grit_based/bf_exps_1_split4/bf_10cls_basic_aug_dmsonorm_750e_sgd_seed21/ResNet_resnet50 True\n",
      "Epoch: 0, Loss: 0.0255980429945478, Accuracy: 0.5390127388535032\n",
      "Epoch: 50, Loss: 0.0023090510813009208, Accuracy: 0.7189490445859873\n",
      "Epoch: 100, Loss: 0.0011745406890133763, Accuracy: 0.7292993630573248\n",
      "Epoch: 150, Loss: 0.0013005987082602111, Accuracy: 0.7460191082802548\n",
      "Epoch: 200, Loss: 0.00085649359229786, Accuracy: 0.7460191082802548\n",
      "Epoch: 250, Loss: 0.0010949772883467614, Accuracy: 0.7261146496815286\n",
      "Epoch: 300, Loss: 0.0008389949650284211, Accuracy: 0.7444267515923567\n",
      "Epoch: 350, Loss: 0.0006100030016794706, Accuracy: 0.7523885350318471\n",
      "Epoch: 400, Loss: 0.0013168727412558854, Accuracy: 0.75\n",
      "Epoch: 450, Loss: 0.0004984400899499465, Accuracy: 0.7627388535031847\n",
      "Best Accuracy: 0.7627388535031847\n",
      "/proj/haste_berzelius/exps/specs_non_grit_based/bf_exps_1_split5/bf_10cls_basic_aug_dmsonorm_750e_sgd_seed21/ResNet_resnet50 True\n",
      "Epoch: 0, Loss: 0.014011020300145664, Accuracy: 0.4863588667366212\n",
      "Epoch: 50, Loss: 0.000895312691160164, Accuracy: 0.7397691500524659\n",
      "Epoch: 100, Loss: 0.00030081480026151303, Accuracy: 0.7376705141657922\n",
      "Epoch: 150, Loss: 0.00018512354753246962, Accuracy: 0.7465897166841553\n",
      "Epoch: 200, Loss: 0.00019278593853913473, Accuracy: 0.75498426023085\n",
      "Epoch: 250, Loss: 0.00014985276040194043, Accuracy: 0.7665267576075551\n",
      "Epoch: 300, Loss: 8.503146076737146e-05, Accuracy: 0.7591815320041972\n",
      "Epoch: 350, Loss: 0.00024020352233728983, Accuracy: 0.7497376705141658\n",
      "Epoch: 400, Loss: 0.00012451114527810782, Accuracy: 0.7602308499475341\n",
      "Epoch: 450, Loss: 0.00010725440360226542, Accuracy: 0.7612801678908709\n",
      "Best Accuracy: 0.7665267576075551\n"
     ]
    }
   ],
   "source": [
    "cp_df_cell = pd.read_csv(\"stats/non_grit_based/CP_features_cells.csv\")\n",
    "site_conversion = pd.DataFrame(\n",
    "    {\"bf_sites\": [\"s1\", \"s2\", \"s3\", \"s4\", \"s5\"], \"f_sites\": [\"s2\", \"s4\", \"s5\", \"s6\", \"s8\"]}\n",
    ")\n",
    "\n",
    "feature_groups = [\n",
    "    \"AreaShape\",\n",
    "    \"Correlation\",\n",
    "    \"Granularity\",\n",
    "    \"Intensity\",\n",
    "    \"Neighbors\",\n",
    "    \"RadialDistribution\",\n",
    "]\n",
    "cp_feature_columns = [c for c in cp_df_cell.columns if c.startswith(tuple(feature_groups))]\n",
    "\n",
    "for bf_folder in bf_folders:\n",
    "    exp_folder = os.path.join(bf_folder, exp_cat, \"ResNet_resnet50\")\n",
    "    print(exp_folder, os.path.exists(exp_folder))\n",
    "    if os.path.exists(exp_folder):\n",
    "        if os.path.isfile(os.path.join(exp_folder, \"feature_data_train.csv\")):\n",
    "            model_train_df = pd.read_csv(os.path.join(exp_folder, \"feature_data_train.csv\"))\n",
    "            model_train_df = model_train_df[model_train_df[\"moa\"] != \"dmso\"]\n",
    "            model_train_df[\"site\"] = model_train_df[\"site\"].map(\n",
    "                site_conversion.set_index(\"bf_sites\")[\"f_sites\"]\n",
    "            )\n",
    "            cp_bf_df = pd.merge(\n",
    "                model_train_df, cp_df_cell, on=[\"plate\", \"well\", \"compound\", \"moa\", \"site\"]\n",
    "            )\n",
    "            cp_train_df = cp_bf_df[cp_df_cell.columns]\n",
    "\n",
    "            model_test_df = pd.read_csv(os.path.join(exp_folder, \"feature_data_test.csv\"))\n",
    "            model_test_df = model_test_df[model_test_df[\"moa\"] != \"dmso\"]\n",
    "            model_test_df[\"site\"] = model_test_df[\"site\"].map(\n",
    "                site_conversion.set_index(\"bf_sites\")[\"f_sites\"]\n",
    "            )\n",
    "            cp_bf_df = pd.merge(\n",
    "                model_test_df, cp_df_cell, on=[\"plate\", \"well\", \"compound\", \"moa\", \"site\"]\n",
    "            )\n",
    "            cp_test_df = cp_bf_df[cp_df_cell.columns]\n",
    "\n",
    "            test_unique_moas = np.unique(cp_test_df[\"moa\"])\n",
    "            n_labels = len(test_unique_moas)\n",
    "\n",
    "            le = preprocessing.LabelEncoder()\n",
    "            le.fit(cp_train_df[\"moa\"])\n",
    "\n",
    "            cp_train_df[\"moa_label\"] = cp_train_df[\"moa\"].apply(lambda x: le.transform([x])[0].item())\n",
    "            cp_test_df[\"moa_label\"] = cp_test_df[\"moa\"].apply(lambda x: le.transform([x])[0].item())\n",
    "\n",
    "            train_X = cp_train_df[cp_feature_columns].values\n",
    "            imputer = impute.SimpleImputer(missing_values=np.nan, strategy=\"mean\").fit(train_X)\n",
    "            train_X = imputer.transform(train_X)\n",
    "            normalize = preprocessing.StandardScaler().fit(train_X)\n",
    "            train_X = normalize.transform(train_X)\n",
    "            train_Y = cp_train_df[\"moa_label\"].values\n",
    "\n",
    "            test_X = cp_test_df[cp_feature_columns].values\n",
    "            test_X = imputer.transform(test_X)\n",
    "            test_X = normalize.transform(test_X)\n",
    "            test_Y = cp_test_df[\"moa_label\"].values\n",
    "\n",
    "        # umap = UMAP(n_components=2, random_state=42).fit(train_X)\n",
    "        # trans = umap.transform(test_X)\n",
    "        # df = pd.DataFrame(\n",
    "        #     {\n",
    "        #         \"x\": trans[:, 0],\n",
    "        #         \"y\": trans[:, 1],\n",
    "        #         \"moa\": cp_test_df[\"moa\"].values,\n",
    "        #     }\n",
    "        # )\n",
    "        # fig, ax = plt.subplots(1, figsize=(16, 12))\n",
    "        # sns.scatterplot(\n",
    "        #     x=\"x\",\n",
    "        #     y=\"y\",\n",
    "        #     hue=\"moa\",\n",
    "        #     palette=sns.color_palette(cc.glasbey, len(np.unique(cp_test_df[\"moa\"].values))),\n",
    "        #     legend=\"brief\",\n",
    "        #     s=100,\n",
    "        #     alpha=0.9,\n",
    "        #     data=df,\n",
    "        # )\n",
    "        # plt.savefig(os.path.join(exp_folder, \"test_embed_moa_cp.png\"), dpi=500)\n",
    "        # plt.close()\n",
    "\n",
    "            outputs, targets = train_cls(\n",
    "                exp_folder, n_labels, train_X, train_Y, test_X, test_Y, \"cp\", \"cp_moa.pth\"\n",
    "            )\n",
    "\n",
    "            pred_moa = le.inverse_transform(np.concatenate(outputs))\n",
    "            moa = le.inverse_transform(np.concatenate(targets))\n",
    "\n",
    "            report = metrics.classification_report(moa, pred_moa, output_dict=True)\n",
    "            report_df = pd.DataFrame(report).transpose()\n",
    "            report_df.to_csv(os.path.join(exp_folder, \"cp_moa_report.csv\"))\n",
    "\n",
    "            df = pd.DataFrame(\n",
    "                {\n",
    "                    \"plate\": cp_test_df[\"plate\"].values,\n",
    "                    \"well\": cp_test_df[\"well\"].values,\n",
    "                    \"site\": cp_test_df[\"site\"].values,\n",
    "                    \"compound\": cp_test_df[\"compound\"].values,\n",
    "                    \"moa\": moa,\n",
    "                    \"pred_moa\": pred_moa,\n",
    "                }\n",
    "            )\n",
    "            df.to_csv(os.path.join(exp_folder, \"cp_moa_analysis.csv\"), index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('deepLtorch3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ffb4ee6da7dff732a4458ae09a79ec797a19ff182b1b136a6b943d2e13294873"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
