{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn import preprocessing, decomposition, metrics, impute\n",
    "import torch, torchvision\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def train_cls(exp_folder, n_labels, train_X, train_Y, test_X, test_Y, mode, model_save_path):\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(672 if mode == \"cp\" else 2048, 256),\n",
    "        nn.BatchNorm1d(256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(256, n_labels),\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters())\n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(train_X).float(), torch.from_numpy(train_Y).long()\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "    test_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(test_X).float(), torch.from_numpy(test_Y).long()\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "    best_accuracy = 0\n",
    "    accuracies = []\n",
    "    for i in range(500):\n",
    "        model.train()\n",
    "        running_total_loss = 0\n",
    "        for _, sample in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            op = model(sample[0])\n",
    "            loss = criterion(op, sample[1])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_total_loss += loss.item()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            model.eval()\n",
    "            accuracy = 0\n",
    "            for _, sample in enumerate(test_loader):\n",
    "                op = model(sample[0])\n",
    "                loss = criterion(op, sample[1])\n",
    "                accuracy += (torch.argmax(op, dim=1) == sample[1]).sum().item()\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                torch.save(model.state_dict(), os.path.join(exp_folder, model_save_path))\n",
    "            print(\n",
    "                \"Epoch: {}, Loss: {}, Accuracy: {}\".format(\n",
    "                    i,\n",
    "                    running_total_loss / len(test_loader.dataset),\n",
    "                    accuracy / len(test_loader.dataset),\n",
    "                )\n",
    "            )\n",
    "            accuracies.append(accuracy / len(test_loader.dataset))\n",
    "\n",
    "    print(\"Best Accuracy:\", np.max(accuracies))\n",
    "    model.load_state_dict(torch.load(os.path.join(exp_folder, model_save_path)))\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        outputs = []\n",
    "        targets = []\n",
    "        for _, sample in enumerate(test_loader):\n",
    "            op = model(sample[0])\n",
    "            outputs.append(torch.argmax(op, dim=1).cpu().numpy())\n",
    "            targets.append(sample[1].cpu().numpy())\n",
    "    return outputs, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf_folders = sorted(glob.glob(\"/proj/haste_berzelius/exps/specs_non_grit_based/*bf*\"))[:5]\n",
    "exp_cat = \"bf_11cls_basic_aug_dmsonorm_750e_sgd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/proj/haste_berzelius/exps/specs_non_grit_based/bf_exps_1_split1/bf_11cls_basic_aug_dmsonorm_750e_sgd/ResNet_resnet50\n",
      "Epoch: 0, Loss: 0.024468150028892532, Accuracy: 0.5632424877707897\n",
      "Epoch: 50, Loss: 0.0012802896665827033, Accuracy: 0.7484276729559748\n",
      "Epoch: 100, Loss: 0.0007091910232725384, Accuracy: 0.7540181691125087\n",
      "Epoch: 150, Loss: 0.0003640659649385846, Accuracy: 0.756114605171209\n",
      "Epoch: 200, Loss: 0.00023983435244524003, Accuracy: 0.7596086652690426\n",
      "Epoch: 250, Loss: 0.0003011822628910353, Accuracy: 0.7686932215234102\n",
      "Epoch: 300, Loss: 0.00018357050297280694, Accuracy: 0.7575122292103424\n",
      "Epoch: 350, Loss: 0.00020603578313820685, Accuracy: 0.7672955974842768\n",
      "Epoch: 400, Loss: 0.00016401730579337733, Accuracy: 0.76659678546471\n",
      "Epoch: 450, Loss: 0.00026435094835414993, Accuracy: 0.7721872816212438\n",
      "Best Accuracy: 0.7721872816212438\n",
      "/proj/haste_berzelius/exps/specs_non_grit_based/bf_exps_1_split2/bf_11cls_basic_aug_dmsonorm_750e_sgd/ResNet_resnet50\n",
      "Epoch: 0, Loss: 0.023722903976023522, Accuracy: 0.5669398907103825\n",
      "Epoch: 50, Loss: 0.0017496056732584219, Accuracy: 0.7848360655737705\n",
      "Epoch: 100, Loss: 0.000711958794320216, Accuracy: 0.8148907103825137\n",
      "Epoch: 150, Loss: 0.0005791946632349914, Accuracy: 0.8114754098360656\n",
      "Epoch: 200, Loss: 0.0004152727219189614, Accuracy: 0.8101092896174863\n",
      "Epoch: 250, Loss: 0.00040499180685118086, Accuracy: 0.8142076502732241\n",
      "Epoch: 300, Loss: 0.0002715469476680368, Accuracy: 0.8162568306010929\n",
      "Epoch: 350, Loss: 0.00024311865719360977, Accuracy: 0.8128415300546448\n",
      "Epoch: 400, Loss: 0.0003604696232754269, Accuracy: 0.8080601092896175\n",
      "Epoch: 450, Loss: 0.00027506807729947927, Accuracy: 0.8005464480874317\n",
      "Best Accuracy: 0.8162568306010929\n",
      "/proj/haste_berzelius/exps/specs_non_grit_based/bf_exps_1_split3/bf_11cls_basic_aug_dmsonorm_750e_sgd/ResNet_resnet50\n",
      "Epoch: 0, Loss: 0.023922160733814785, Accuracy: 0.5211952744961779\n",
      "Epoch: 50, Loss: 0.0016126642683216728, Accuracy: 0.7623349548297429\n",
      "Epoch: 100, Loss: 0.000852668597061987, Accuracy: 0.7658095899930507\n",
      "Epoch: 150, Loss: 0.0005102237366797445, Accuracy: 0.7706740792216817\n",
      "Epoch: 200, Loss: 0.0007062422757537271, Accuracy: 0.7804030576789437\n",
      "Epoch: 250, Loss: 0.0004492899412036853, Accuracy: 0.7678943710910354\n",
      "Epoch: 300, Loss: 0.00028055065581374404, Accuracy: 0.7741487143849896\n",
      "Epoch: 350, Loss: 0.00024350306544714375, Accuracy: 0.7797081306462822\n",
      "Epoch: 400, Loss: 0.00032008093729345974, Accuracy: 0.7908269631688672\n",
      "Epoch: 450, Loss: 0.0001900601495235972, Accuracy: 0.7845726198749131\n",
      "Best Accuracy: 0.7908269631688672\n",
      "/proj/haste_berzelius/exps/specs_non_grit_based/bf_exps_1_split4/bf_11cls_basic_aug_dmsonorm_750e_sgd/ResNet_resnet50\n",
      "Epoch: 0, Loss: 0.02485437936105436, Accuracy: 0.5522284122562674\n",
      "Epoch: 50, Loss: 0.0017574201999517537, Accuracy: 0.7270194986072424\n",
      "Epoch: 100, Loss: 0.0007470983669452348, Accuracy: 0.7416434540389972\n",
      "Epoch: 150, Loss: 0.0005493036089957923, Accuracy: 0.7479108635097493\n",
      "Epoch: 200, Loss: 0.0004418080420983037, Accuracy: 0.7576601671309192\n",
      "Epoch: 250, Loss: 0.0004891419230667164, Accuracy: 0.7520891364902507\n",
      "Epoch: 300, Loss: 0.00043420949470884785, Accuracy: 0.7444289693593314\n",
      "Epoch: 350, Loss: 0.00021281070302922827, Accuracy: 0.7513927576601671\n",
      "Epoch: 400, Loss: 0.00042228966634876215, Accuracy: 0.7520891364902507\n",
      "Epoch: 450, Loss: 0.00023246469841310156, Accuracy: 0.7534818941504178\n",
      "Best Accuracy: 0.7576601671309192\n",
      "/proj/haste_berzelius/exps/specs_non_grit_based/bf_exps_1_split5/bf_11cls_basic_aug_dmsonorm_750e_sgd/ResNet_resnet50\n",
      "Epoch: 0, Loss: 0.015190118339809223, Accuracy: 0.49760306807286675\n",
      "Epoch: 50, Loss: 0.0007467002287321328, Accuracy: 0.7387344199424737\n",
      "Epoch: 100, Loss: 0.00042288071875061305, Accuracy: 0.7430488974113135\n",
      "Epoch: 150, Loss: 0.0002083304728366156, Accuracy: 0.7588686481303931\n",
      "Epoch: 200, Loss: 0.00028194031434371165, Accuracy: 0.7574304889741131\n",
      "Epoch: 250, Loss: 0.0002416471414060858, Accuracy: 0.7545541706615532\n",
      "Epoch: 300, Loss: 0.00013357770699768372, Accuracy: 0.7440076701821668\n",
      "Epoch: 350, Loss: 0.0001546036590905083, Accuracy: 0.7564717162032598\n",
      "Epoch: 400, Loss: 8.548149988162381e-05, Accuracy: 0.763183125599233\n",
      "Epoch: 450, Loss: 0.00011382590039978777, Accuracy: 0.7598274209012464\n",
      "Best Accuracy: 0.763183125599233\n"
     ]
    }
   ],
   "source": [
    "cp_df_cell = pd.read_csv(\"stats/non_grit_based/CP_features_cells.csv\")\n",
    "site_conversion = pd.DataFrame(\n",
    "    {\"bf_sites\": [\"s1\", \"s2\", \"s3\", \"s4\", \"s5\"], \"f_sites\": [\"s2\", \"s4\", \"s5\", \"s6\", \"s8\"]}\n",
    ")\n",
    "\n",
    "feature_groups = [\n",
    "    \"AreaShape\",\n",
    "    \"Correlation\",\n",
    "    \"Granularity\",\n",
    "    \"Intensity\",\n",
    "    \"Neighbors\",\n",
    "    \"RadialDistribution\",\n",
    "]\n",
    "cp_feature_columns = [c for c in cp_df_cell.columns if c.startswith(tuple(feature_groups))]\n",
    "\n",
    "for bf_folder in bf_folders:\n",
    "    exp_folder = os.path.join(bf_folder, \"bf_11cls_basic_aug_dmsonorm_750e_sgd\", \"ResNet_resnet50\")\n",
    "    print(exp_folder)\n",
    "    if os.path.exists(exp_folder):\n",
    "        model_train_df = pd.read_csv(os.path.join(exp_folder, \"feature_data_train.csv\"))\n",
    "        model_train_df[\"site\"] = model_train_df[\"site\"].map(\n",
    "            site_conversion.set_index(\"bf_sites\")[\"f_sites\"]\n",
    "        )\n",
    "        cp_bf_df = pd.merge(\n",
    "            model_train_df, cp_df_cell, on=[\"plate\", \"well\", \"compound\", \"moa\", \"site\"]\n",
    "        )\n",
    "        cp_train_df = cp_bf_df[cp_df_cell.columns]\n",
    "\n",
    "        model_test_df = pd.read_csv(os.path.join(exp_folder, \"feature_data_test.csv\"))\n",
    "        model_test_df[\"site\"] = model_test_df[\"site\"].map(\n",
    "            site_conversion.set_index(\"bf_sites\")[\"f_sites\"]\n",
    "        )\n",
    "        cp_bf_df = pd.merge(\n",
    "            model_test_df, cp_df_cell, on=[\"plate\", \"well\", \"compound\", \"moa\", \"site\"]\n",
    "        )\n",
    "        cp_test_df = cp_bf_df[cp_df_cell.columns]\n",
    "\n",
    "        test_unique_moas = np.unique(cp_test_df[\"moa\"])\n",
    "        n_labels = len(test_unique_moas)\n",
    "\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le.fit(cp_train_df[\"moa\"])\n",
    "\n",
    "        cp_train_df[\"moa_label\"] = cp_train_df[\"moa\"].apply(lambda x: le.transform([x])[0].item())\n",
    "        cp_test_df[\"moa_label\"] = cp_test_df[\"moa\"].apply(lambda x: le.transform([x])[0].item())\n",
    "\n",
    "        train_X = cp_train_df[cp_feature_columns].values\n",
    "        imputer = impute.SimpleImputer(missing_values=np.nan, strategy=\"mean\").fit(train_X)\n",
    "        train_X = imputer.transform(train_X)\n",
    "        normalize = preprocessing.StandardScaler().fit(train_X)\n",
    "        train_X = normalize.transform(train_X)\n",
    "        train_Y = cp_train_df[\"moa_label\"].values\n",
    "\n",
    "        test_X = cp_test_df[cp_feature_columns].values\n",
    "        test_X = imputer.transform(test_X)\n",
    "        test_X = normalize.transform(test_X)\n",
    "        test_Y = cp_test_df[\"moa_label\"].values\n",
    "\n",
    "        outputs, targets = train_cls(\n",
    "            exp_folder, n_labels, train_X, train_Y, test_X, test_Y, \"cp\", \"cp_moa.pth\"\n",
    "        )\n",
    "\n",
    "        pred_moa = le.inverse_transform(np.concatenate(outputs))\n",
    "        moa = le.inverse_transform(np.concatenate(targets))\n",
    "\n",
    "        report = metrics.classification_report(moa, pred_moa, output_dict=True)\n",
    "        report_df = pd.DataFrame(report).transpose()\n",
    "        report_df.to_csv(os.path.join(exp_folder, \"cp_moa_report.csv\"))\n",
    "\n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                \"plate\": cp_test_df[\"plate\"].values,\n",
    "                \"well\": cp_test_df[\"well\"].values,\n",
    "                \"site\": cp_test_df[\"site\"].values,\n",
    "                \"compound\": cp_test_df[\"compound\"].values,\n",
    "                \"moa\": moa,\n",
    "                \"pred_moa\": pred_moa,\n",
    "            }\n",
    "        )\n",
    "        df.to_csv(os.path.join(exp_folder, \"cp_moa_analysis.csv\"), index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('deepLtorch3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ffb4ee6da7dff732a4458ae09a79ec797a19ff182b1b136a6b943d2e13294873"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
